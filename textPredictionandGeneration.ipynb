{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGvPWnMTT7A1",
    "outputId": "7a9516f0-e274-4ac2-d067-e89ceca1570a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive._mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxiZ42B4SwQ-"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from tests import test_prediction, test_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GU4_8F4qUyy2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_softmax(x, axis):\n",
    "    ret = x - np.max(x, axis=axis, keepdims=True)\n",
    "    lsm = np.log(np.sum(np.exp(ret), axis=axis, keepdims=True))\n",
    "    return ret - lsm\n",
    "\n",
    "\n",
    "def array_to_str(arr, vocab):\n",
    "    # print('arr',arr)\n",
    "    return \" \".join(vocab[a] for a in arr)\n",
    "\n",
    "\n",
    "def test_prediction(out, targ):\n",
    "    # out = out.to('cpu')\n",
    "    out = log_softmax(out, 1)\n",
    "    nlls = out[np.arange(out.shape[0]), targ]\n",
    "    nll = -np.mean(nlls)\n",
    "    return nll\n",
    "\n",
    "def test_generation(inp, pred, vocab):\n",
    "    outputs = u\"\"\n",
    "    for i in range(inp.shape[0]):\n",
    "        # print('input i', inp[i])\n",
    "        w1 = array_to_str(inp[i], vocab)\n",
    "        # print('after w1')\n",
    "        # print('pred i', pred[i])\n",
    "        w2 = array_to_str(pred[i], vocab)\n",
    "        # print('after w2')\n",
    "        outputs += u\"Input | Output #{}: {} | {}\\n\".format(i, w1, w2)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5znxQhLSwRC"
   },
   "outputs": [],
   "source": [
    "# load all that we need\n",
    "\n",
    "dataset = np.load('/content/gdrive/MyDrive/IDL-HW4-P1/dataset/wiki.train.npy', allow_pickle=True)\n",
    "devset = np.load('/content/gdrive/MyDrive/IDL-HW4-P1/dataset/wiki.valid.npy', allow_pickle=True)\n",
    "fixtures_pred = np.load('/content/gdrive/MyDrive/IDL-HW4-P1/fixtures/prediction.npz')  # dev - 2 keys - inp,out\n",
    "fixtures_gen = np.load('/content/gdrive/MyDrive/IDL-HW4-P1/fixtures/generation.npy')  # dev\n",
    "fixtures_pred_test = np.load('/content/gdrive/MyDrive/IDL-HW4-P1/fixtures/prediction_test.npz')  # test\n",
    "fixtures_gen_test = np.load('/content/gdrive/MyDrive/IDL-HW4-P1/fixtures/generation_test.npy')  # test\n",
    "vocab = np.load('/content/gdrive/MyDrive/IDL-HW4-P1/dataset/vocab.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZNrJ8XvSwRF"
   },
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "class LanguageModelDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset = np.concatenate(dataset)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.sequence_len = 50\n",
    "#         raise NotImplemented\n",
    "\n",
    "    def __iter__(self):\n",
    "        # concatenate your articles and build into batches\n",
    "        seq_data = []\n",
    "        seq_label = []\n",
    "        j = 0\n",
    "#         for i in range(self.dataset.shape[0]-self.sequence_len-1):\n",
    "#             data = self.dataset[i:i+self.sequence_len],self.dataset[i+1:i+1+self.sequence_len]\n",
    "#             seq_data.append(data)\n",
    "#             j+=1\n",
    "#             if j == self.batch_size:\n",
    "#                 j = 0\n",
    "#                 yield seq_data[-self.batch_size:]\n",
    "\n",
    "        for i in range(0,self.dataset.shape[0]-self.sequence_len-1,self.sequence_len):\n",
    "            data = self.dataset[i:i+self.sequence_len]\n",
    "            seq_data.append(data)\n",
    "            label = self.dataset[i+1:i+1+self.sequence_len]\n",
    "            seq_label.append(label)\n",
    "            j+=1\n",
    "            if j == self.batch_size:\n",
    "                j = 0\n",
    "                yield (seq_data[-self.batch_size:],seq_label[-self.batch_size:])\n",
    "                \n",
    "        # raise NotImplemented\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pp0piW8XkiEK"
   },
   "outputs": [],
   "source": [
    "class LockedDropout(nn.Module):\n",
    "    \"\"\" LockedDropout applies the same dropout mask to every time step.\n",
    "\n",
    "    **Thank you** to Sales Force for their initial implementation of :class:`WeightDrop`. Here is\n",
    "    their `License\n",
    "    <https://github.com/salesforce/awd-lstm-lm/blob/master/LICENSE>`__.\n",
    "\n",
    "    Args:\n",
    "        p (float): Probability of an element in the dropout mask to be zeroed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (:class:`torch.FloatTensor` [sequence length, batch size, rnn hidden size]): Input to\n",
    "                apply dropout too.\n",
    "        \"\"\"\n",
    "        if not self.training or not self.p:\n",
    "            return x\n",
    "        x = x.clone()\n",
    "        mask = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - self.p)\n",
    "        # mask = mask.long()\n",
    "        mask = mask.div_(1 - self.p)\n",
    "        mask = mask.expand_as(x)\n",
    "        return x * mask\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'p=' + str(self.p) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zt-7YsTYSwRI"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "class LanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "        TODO: Define your model here\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        embedding_size = 32\n",
    "        input_size = embedding_size\n",
    "        hidden_size = 256\n",
    "        hidden_size_2 = 512\n",
    "        self.emb = torch.nn.Embedding(self.vocab_size,embedding_size)\n",
    "        self.drop1 = LockedDropout(0.5)\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, bidirectional=False)\n",
    "        self.drop2 = nn.Dropout(0.5) #LockedDropout(0.5)\n",
    "        self.lstm2 = nn.LSTM(input_size=int(hidden_size), hidden_size=int(hidden_size_2), num_layers=1, bidirectional=False)\n",
    "        self.drop3 = nn.Dropout(0.5) #LockedDropout(0.5)\n",
    "        self.lstm3 = nn.LSTM(input_size=int(hidden_size_2), hidden_size=int(hidden_size_2*2), num_layers=1, bidirectional=False)\n",
    "        # self.drop4 = LockedDropout(0.5)\n",
    "        # self.lstm4 = nn.LSTM(input_size=hidden_size_2*8, hidden_size=hidden_size_2*16, num_layers=1, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(int(hidden_size_2*2), self.vocab_size)\n",
    "        # raise NotImplemented\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
    "        # raise NotImplemented\n",
    "        # print(x)\n",
    "        # x = np.array(x)\n",
    "        # print(x.shape)\n",
    "        # x = x.reshape((x.shape[0],x.shape[1],1))\n",
    "        x = self.emb(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        # print('After permute 1',x.shape)\n",
    "        x = self.drop1(x)\n",
    "        x = self.lstm1(x)[0]\n",
    "        # print('After lstm 1',x.shape)\n",
    "        x = self.drop2(x)\n",
    "        x = self.lstm2(x)[0]\n",
    "        # print('After lstm 2',x.shape)\n",
    "        x = self.drop3(x)\n",
    "        x = self.lstm3(x)[0]\n",
    "        # print('After lstm 3',x.shape)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        # print('After permute 2',x.shape)\n",
    "        # x = x[:,-1,:]\n",
    "        x = self.fc1(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIvZOIfjSwRK"
   },
   "outputs": [],
   "source": [
    "# model trainer\n",
    "\n",
    "class LanguageModelTrainer:\n",
    "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
    "        \"\"\"\n",
    "            Use this class to train your model\n",
    "        \"\"\"\n",
    "        # feel free to add any other parameters here\n",
    "        self.model = model.to('cuda')\n",
    "        self.loader = loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions = []\n",
    "        self.predictions_test = []\n",
    "        self.generated_logits = []\n",
    "        self.generated = []\n",
    "        self.generated_logits_test = []\n",
    "        self.generated_test = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        # TODO: Define your optimizer and criterion here\n",
    "        self.optimizer = torch.optim.ASGD(model.parameters(), lr=0.1, weight_decay=0.1)\n",
    "        # self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
    "            inputs = torch.tensor(np.array(inputs)).to('cuda')\n",
    "            targets = torch.LongTensor(targets).to('cuda')\n",
    "            # print('Shape of targets', targets.shape)\n",
    "            # print(model.vocab_size)\n",
    "            # targets = torch.nn.functional.one_hot(targets,model.vocab_size)\n",
    "            # targets = nn.Embedding(model.vocab_size,model.vocab_size)(targets)\n",
    "            # targets = targets.type(torch.LongTensor)\n",
    "            if batch_num % 10 == 9:\n",
    "              print('Processed %d batches'%(batch_num+1))\n",
    "            epoch_loss += self.train_batch(inputs, targets)\n",
    "        # self.scheduler.step()\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.epochs += 1\n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "    def train_batch(self, inputs, targets):\n",
    "        \"\"\" \n",
    "            TODO: Define code for training a single batch of inputs\n",
    "        \"\"\"\n",
    "        out = model(inputs) #.float()\n",
    "        # print('shape of preds',out.shape)\n",
    "        # print('shape of targets', targets.shape)\n",
    "        loss = self.criterion(out,targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "        # raise NotImplemented\n",
    "\n",
    "    \n",
    "    def test(self):\n",
    "        # don't change these\n",
    "        self.model.eval() # set to eval mode\n",
    "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
    "        self.predictions.append(predictions)\n",
    "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
    "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)\n",
    "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
    "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
    "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
    "        self.val_losses.append(nll)\n",
    "        \n",
    "        self.generated.append(generated)\n",
    "        self.generated_test.append(generated_test)\n",
    "        self.generated_logits.append(generated_logits)\n",
    "        self.generated_logits_test.append(generated_logits_test)\n",
    "        \n",
    "        # generate predictions for test data\n",
    "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
    "        self.predictions_test.append(predictions_test)\n",
    "            \n",
    "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, nll))\n",
    "        return nll\n",
    "\n",
    "    def save(self):\n",
    "        # don't change these\n",
    "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
    "        torch.save({'state_dict': self.model.state_dict()},\n",
    "            model_path)\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated_test[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPI7_kZRSwRN"
   },
   "outputs": [],
   "source": [
    "class TestLanguageModel:\n",
    "    def prediction(inp, model):\n",
    "        \"\"\"\n",
    "            TODO: write prediction code here\n",
    "            \n",
    "            :param inp:\n",
    "            :return: a np.ndarray of logits\n",
    "        \"\"\"\n",
    "        # model = model.to('cuda')\n",
    "        inp = torch.tensor(inp).to('cuda')\n",
    "        out = model(inp)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        out = out[:,:,-1]\n",
    "        out = np.reshape(out,(out.shape[0],out.shape[1]))\n",
    "        print('output shape in prediction is', out.shape)\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def generation(inp, forward, model):\n",
    "        \"\"\"\n",
    "            TODO: write generation code here\n",
    "\n",
    "            Generate a sequence of words given a starting sequence.\n",
    "            :param inp: Initial sequence of words (batch size, length)\n",
    "            :param forward: number of additional words to generate\n",
    "            :return: generated words (batch size, forward)\n",
    "        \"\"\"  \n",
    "        inp = torch.tensor(inp).to('cuda')\n",
    "        out = model(inp)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        out = out[:,:,-forward:]\n",
    "        # out = np.reshape(out,(out.shape[0],out.shape[1]))\n",
    "        out = np.argmax(out,axis=1)\n",
    "        print('output shape in generation is', out.shape)\n",
    "        print('unique values',np.unique(out))\n",
    "        return out      \n",
    "        # raise NotImplemented\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiUrjbEjSwRQ"
   },
   "outputs": [],
   "source": [
    "# TODO: define other hyperparameters here\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2HCVG5YISwRW",
    "outputId": "31c5e408-26f1-4843-ef33-02272f9ba037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models, predictions, and generated words to ./experiments/1639073926\n"
     ]
    }
   ],
   "source": [
    "run_id = str(int(time.time()))\n",
    "if not os.path.exists('./experiments'):\n",
    "    os.mkdir('./experiments')\n",
    "os.mkdir('./experiments/%s' % run_id)\n",
    "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nrjpp-xu5jvD"
   },
   "outputs": [],
   "source": [
    "# dataset[:60].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHUPsgqxppUU"
   },
   "outputs": [],
   "source": [
    "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbHH6zXTSwRa"
   },
   "outputs": [],
   "source": [
    "model = LanguageModel(len(vocab))\n",
    "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7D8wTJkBSwRc",
    "outputId": "3ddbc227-6527-4d4c-9fb2-53c1bbf5f35d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n",
      "Processed 80 batches\n",
      "Processed 90 batches\n",
      "Processed 100 batches\n",
      "Processed 110 batches\n",
      "Processed 120 batches\n",
      "Processed 130 batches\n",
      "Processed 140 batches\n",
      "Processed 150 batches\n",
      "Processed 160 batches\n",
      "[TRAIN]  Epoch [22/20]   Loss: 6.0113\n",
      "output shape in prediction is (128, 33278)\n",
      "output shape in generation is (32, 10)\n",
      "unique values [    1    72    73    76    79    87    88  1417  1419  1420  1424  1425\n",
      " 13276 14549 14606 14658 15340 16134 16176 16802 21201 21626 22200 22968\n",
      " 23592 23956 25821 31352 31353 31543 32747]\n",
      "output shape in generation is (128, 10)\n",
      "unique values [    1    72    73    76    79    86    88  1417  1419  1420  1423  1424\n",
      "  1425  1821  7012  8204 13276 13774 14118 14549 14658 15219 15340 16134\n",
      " 16176 16802 17154 18779 20243 20398 21201 21626 22200 22968 23592 23816\n",
      " 23956 24452 24958 25639 25821 26190 29092 29294 30547 31348 31352 31353\n",
      " 31514 31543 32747 32846]\n",
      "output shape in prediction is (128, 33278)\n",
      "[VAL]  Epoch [22/20]   Loss: 5.3887\n",
      "Saving model, predictions and generated output for epoch 0 with NLL: 5.3886957\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n",
      "Processed 80 batches\n",
      "Processed 90 batches\n",
      "Processed 100 batches\n",
      "Processed 110 batches\n",
      "Processed 120 batches\n",
      "Processed 130 batches\n",
      "Processed 140 batches\n",
      "Processed 150 batches\n",
      "Processed 160 batches\n",
      "[TRAIN]  Epoch [23/20]   Loss: 5.9864\n",
      "output shape in prediction is (128, 33278)\n",
      "output shape in generation is (32, 10)\n",
      "unique values [    1    64    72    73    76    79    87    88  1241  1417  1419  1420\n",
      "  1424  1425  4296  7616 13276 14549 14658 15219 15340 16134 16802 19918\n",
      " 21201 21626 22200 22968 23592 25639 25821 31348 31352 31353 31514 31543\n",
      " 32747]\n",
      "output shape in generation is (128, 10)\n",
      "unique values [    1    72    73    76    79    86    88  1417  1419  1420  1423  1424\n",
      "  1425  1821  7012  8204 13276 13774 14549 14658 15219 15340 16134 16176\n",
      " 16802 17154 19918 20398 21201 21626 22200 22213 22968 23592 24452 25204\n",
      " 25639 25821 26190 29092 29294 30547 31348 31352 31353 31514 31543 31891\n",
      " 32747 32846]\n",
      "output shape in prediction is (128, 33278)\n",
      "[VAL]  Epoch [23/20]   Loss: 5.3705\n",
      "Saving model, predictions and generated output for epoch 1 with NLL: 5.3704605\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n",
      "Processed 80 batches\n",
      "Processed 90 batches\n",
      "Processed 100 batches\n",
      "Processed 110 batches\n",
      "Processed 120 batches\n",
      "Processed 130 batches\n",
      "Processed 140 batches\n",
      "Processed 150 batches\n",
      "Processed 160 batches\n",
      "[TRAIN]  Epoch [24/20]   Loss: 5.9496\n",
      "output shape in prediction is (128, 33278)\n",
      "output shape in generation is (32, 10)\n",
      "unique values [    1    64    72    73    76    79    87    88   805  1414  1417  1419\n",
      "  1420  1424  1425  1821  4296  7616 13276 14549 14658 15340 16176 16802\n",
      " 18779 19918 21201 22200 22213 22968 23592 25639 25821 26190 30017 31348\n",
      " 31353 31543 32747 33245]\n",
      "output shape in generation is (128, 10)\n",
      "unique values [    1    64    72    73    76    79    86    88   117   792  1414  1417\n",
      "  1419  1420  1423  1424  1425  1821 13276 13774 14118 14549 14658 15219\n",
      " 15340 15577 15659 16134 16176 16802 17154 18804 19918 20398 21201 22200\n",
      " 22213 22968 23592 23816 24452 25204 25639 25821 26190 29092 29294 30547\n",
      " 31348 31352 31353 31514 31543 31582 31891 32747 32846 33245]\n",
      "output shape in prediction is (128, 33278)\n",
      "[VAL]  Epoch [24/20]   Loss: 5.3780\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n",
      "Processed 80 batches\n",
      "Processed 90 batches\n",
      "Processed 100 batches\n",
      "Processed 110 batches\n",
      "Processed 120 batches\n",
      "Processed 130 batches\n",
      "Processed 140 batches\n",
      "Processed 150 batches\n",
      "Processed 160 batches\n",
      "[TRAIN]  Epoch [25/20]   Loss: 5.9192\n",
      "output shape in prediction is (128, 33278)\n",
      "output shape in generation is (32, 10)\n",
      "unique values [    1    64    72    73    76    79    87    88   805  1417  1419  1420\n",
      "  1424  1425  7616 13276 14549 14658 15340 15659 16176 16802 19918 21201\n",
      " 21626 22200 22968 23592 23612 25639 25821 26190 28010 30017 31352 31353\n",
      " 31543 32747 33245]\n",
      "output shape in generation is (128, 10)\n",
      "unique values [    1    64    72    73    76    79    88   805  1182  1417  1419  1420\n",
      "  1423  1424  1425  1821  7012 13276 13774 14118 14549 14658 15340 15577\n",
      " 15659 16134 16176 16266 16802 17154 19918 20398 21201 22200 22213 22968\n",
      " 23592 23612 24452 24958 25204 25639 25821 26190 29092 29294 30547 31348\n",
      " 31352 31353 31514 31543 31891 32747 32846 32883]\n",
      "output shape in prediction is (128, 33278)\n",
      "[VAL]  Epoch [25/20]   Loss: 5.2955\n",
      "Saving model, predictions and generated output for epoch 3 with NLL: 5.2954617\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n",
      "Processed 80 batches\n",
      "Processed 90 batches\n",
      "Processed 100 batches\n",
      "Processed 110 batches\n",
      "Processed 120 batches\n",
      "Processed 130 batches\n",
      "Processed 140 batches\n",
      "Processed 150 batches\n",
      "Processed 160 batches\n",
      "[TRAIN]  Epoch [26/20]   Loss: 5.8817\n",
      "output shape in prediction is (128, 33278)\n",
      "output shape in generation is (32, 10)\n",
      "unique values [    1    64    72    73    76    79    87    88   805  1417  1419  1420\n",
      "  1424  1425  7616 13276 14549 14658 15219 15340 15659 16176 16802 21201\n",
      " 22200 22213 22968 23592 23612 25525 25639 25821 26190 28010 30017 31348\n",
      " 31352 31353 31543 32747 33245]\n",
      "output shape in generation is (128, 10)\n",
      "unique values [    1    64    72    73    76    79    88  1414  1417  1419  1420  1423\n",
      "  1424  1425  1821  7012  7616 13276 14118 14549 14658 15219 15340 15577\n",
      " 15659 16134 16176 16802 17154 18804 21201 22200 22213 22968 23592 24452\n",
      " 24958 25639 25821 26190 28010 29092 29294 30547 31348 31352 31353 31514\n",
      " 31543 31891 32747 32846 32883]\n",
      "output shape in prediction is (128, 33278)\n",
      "[VAL]  Epoch [26/20]   Loss: 5.3073\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n",
      "Processed 80 batches\n",
      "Processed 90 batches\n",
      "Processed 100 batches\n",
      "Processed 110 batches\n",
      "Processed 120 batches\n",
      "Processed 130 batches\n",
      "Processed 140 batches\n",
      "Processed 150 batches\n",
      "Processed 160 batches\n",
      "[TRAIN]  Epoch [27/20]   Loss: 5.8333\n",
      "output shape in prediction is (128, 33278)\n",
      "output shape in generation is (32, 10)\n",
      "unique values [    1    72    73    76    79    87    88   805  1417  1419  1420  1424\n",
      "  1425  2512  4296  7616  8204 13276 13774 14549 14658 15340 15659 16176\n",
      " 16802 18779 19918 21201 21758 22200 22213 22534 22968 23592 25639 25821\n",
      " 26190 28010 28296 30017 31348 31352 31353 31543 32747 33245]\n",
      "output shape in generation is (128, 10)\n",
      "unique values [    1    64    65    72    73    76    79    88  1414  1417  1419  1420\n",
      "  1423  1424  1425  1821  3020  7616  8204 11562 13276 13774 14549 14658\n",
      " 15340 15577 15578 15659 16134 16176 16802 17154 18804 19918 21201 22200\n",
      " 22213 22968 23592 24382 24452 25204 25639 25821 26190 28296 29092 29294\n",
      " 29456 30547 31348 31352 31353 31543 31891 32747 32846 32883]\n",
      "output shape in prediction is (128, 33278)\n",
      "[VAL]  Epoch [27/20]   Loss: 5.2563\n",
      "Saving model, predictions and generated output for epoch 5 with NLL: 5.2563386\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n",
      "Processed 80 batches\n",
      "Processed 90 batches\n",
      "Processed 100 batches\n",
      "Processed 110 batches\n",
      "Processed 120 batches\n",
      "Processed 130 batches\n",
      "Processed 140 batches\n",
      "Processed 150 batches\n",
      "Processed 160 batches\n",
      "[TRAIN]  Epoch [28/20]   Loss: 5.7890\n",
      "output shape in prediction is (128, 33278)\n",
      "output shape in generation is (32, 10)\n",
      "unique values [    1    64    72    73    76    79    87    88   805  1417  1419  1420\n",
      "  1424  1425  7616 13276 14549 14658 15340 15578 15659 16176 16802 18779\n",
      " 21201 21626 21758 22200 22534 22968 23612 23956 25639 25821 26190 28010\n",
      " 31352 31353 31543 32747 33245]\n",
      "output shape in generation is (128, 10)\n",
      "unique values [    1    64    72    73    76    79    88   946  1182  1414  1417  1419\n",
      "  1420  1423  1424  1425  1821  3020  7610  8204 11562 13276 14118 14549\n",
      " 14658 15219 15340 15578 15659 16134 16176 16802 17154 18804 20398 21201\n",
      " 22213 22968 23592 23842 23956 24382 24452 25204 25262 25639 25821 25881\n",
      " 26190 28010 28296 29294 29456 30547 31348 31352 31353 31543 32747 32846\n",
      " 32883]\n",
      "output shape in prediction is (128, 33278)\n",
      "[VAL]  Epoch [28/20]   Loss: 5.2120\n",
      "Saving model, predictions and generated output for epoch 6 with NLL: 5.2119684\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n",
      "Processed 80 batches\n",
      "Processed 90 batches\n",
      "Processed 100 batches\n",
      "Processed 110 batches\n",
      "Processed 120 batches\n",
      "Processed 130 batches\n",
      "Processed 140 batches\n",
      "Processed 150 batches\n",
      "Processed 160 batches\n",
      "[TRAIN]  Epoch [29/20]   Loss: 5.7453\n",
      "output shape in prediction is (128, 33278)\n",
      "output shape in generation is (32, 10)\n",
      "unique values [    1     4    64    72    73    76    79    87    88   805   946  1417\n",
      "  1419  1420  1424  1425  3020  7616  8204 13276 14549 14658 15219 15340\n",
      " 15659 16176 16802 17154 18779 21201 22534 22968 23592 23612 25525 25639\n",
      " 25821 26190 28010 30017 31348 31352 31353 31543 32747 32899 33245]\n",
      "output shape in generation is (128, 10)\n",
      "unique values [    1    64    65    72    73    76    79    88  1182  1414  1417  1419\n",
      "  1420  1423  1424  1425  1821  2224  3020  7033  7339  7616  8204 11562\n",
      " 13276 14118 14549 14658 15219 15340 15577 15659 16134 16176 16802 17154\n",
      " 18804 19918 20398 21201 22201 22213 22534 22968 23592 24382 24452 25204\n",
      " 25262 25639 25821 26190 28010 28296 29092 29294 30547 31348 31352 31353\n",
      " 31543 31891 32353 32747 32846 33124]\n",
      "output shape in prediction is (128, 33278)\n",
      "[VAL]  Epoch [29/20]   Loss: 5.1878\n",
      "Saving model, predictions and generated output for epoch 7 with NLL: 5.187825\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n",
      "Processed 80 batches\n",
      "Processed 90 batches\n",
      "Processed 100 batches\n",
      "Processed 110 batches\n",
      "Processed 120 batches\n",
      "Processed 130 batches\n",
      "Processed 140 batches\n",
      "Processed 150 batches\n",
      "Processed 160 batches\n",
      "[TRAIN]  Epoch [30/20]   Loss: 5.7153\n",
      "output shape in prediction is (128, 33278)\n",
      "output shape in generation is (32, 10)\n",
      "unique values [    1    64    72    73    76    79    87    88   805   946  1417  1419\n",
      "  1420  1424  1425  1821  3020  7616  8204 13276 13774 14549 14658 15219\n",
      " 15340 15988 16176 16786 16802 18779 21201 22534 22968 23592 25639 25821\n",
      " 25871 26190 28010 28296 29294 31348 31352 31353 31543 32747 32842 32899\n",
      " 33245]\n",
      "output shape in generation is (128, 10)\n",
      "unique values [    1    64    72    73    76    79    88   792   946  1414  1417  1419\n",
      "  1420  1423  1424  1425  1821  3020  7012  7033  7339  7610  7616  8204\n",
      " 11562 13276 13368 14118 14549 14658 15219 15340 15382 15577 15659 16134\n",
      " 16176 16802 17154 18804 19918 20398 21201 22201 22213 22534 22968 23592\n",
      " 24382 24452 24730 25204 25639 25821 25871 26190 28010 29092 29294 30017\n",
      " 30125 30547 31348 31352 31353 31514 31543 31891 32353 32747 32842 32883\n",
      " 32899]\n",
      "output shape in prediction is (128, 33278)\n",
      "[VAL]  Epoch [30/20]   Loss: 5.1477\n",
      "Saving model, predictions and generated output for epoch 8 with NLL: 5.147663\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n",
      "Processed 80 batches\n",
      "Processed 90 batches\n",
      "Processed 100 batches\n",
      "Processed 110 batches\n",
      "Processed 120 batches\n",
      "Processed 130 batches\n",
      "Processed 140 batches\n",
      "Processed 150 batches\n",
      "Processed 160 batches\n",
      "[TRAIN]  Epoch [31/20]   Loss: 5.6974\n",
      "output shape in prediction is (128, 33278)\n",
      "output shape in generation is (32, 10)\n",
      "unique values [    1     4    72    73    76    79    87    88   805   930   946  1417\n",
      "  1419  1420  1424  1425  1821  3020  7616  8843  9483 13276 13774 14549\n",
      " 14655 14658 15219 15340 15578 15659 16176 16786 16802 18779 19918 21201\n",
      " 21758 22346 22534 23592 23612 25639 25821 28010 28296 30017 31348 31352\n",
      " 31353 31543 32747 32899 33245]\n",
      "output shape in generation is (128, 10)\n",
      "unique values [    1    64    65    72    73    76    79    88  1182  1417  1419  1420\n",
      "  1423  1424  1425  1821  3020  5140  6880  7033  7339  7610  7616 11562\n",
      " 12733 13276 13774 14118 14549 14658 15219 15340 15382 15577 15578 15659\n",
      " 16134 16176 16802 17154 18804 19712 19918 19941 20243 20398 21027 21201\n",
      " 22201 22213 22968 23592 23612 23956 23959 24382 24452 25204 25639 25821\n",
      " 25871 26190 28010 28727 29092 29294 30125 30158 30547 31348 31352 31353\n",
      " 31514 31543 31891 32747 32842 32846 32883]\n",
      "output shape in prediction is (128, 33278)\n",
      "[VAL]  Epoch [31/20]   Loss: 5.1987\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n",
      "Processed 80 batches\n",
      "Processed 90 batches\n",
      "Processed 100 batches\n",
      "Processed 110 batches\n",
      "Processed 120 batches\n",
      "Processed 130 batches\n",
      "Processed 140 batches\n",
      "Processed 150 batches\n",
      "Processed 160 batches\n",
      "[TRAIN]  Epoch [32/20]   Loss: 5.6691\n",
      "output shape in prediction is (128, 33278)\n",
      "output shape in generation is (32, 10)\n",
      "unique values [    1     4    64    72    73    76    79    87    88   117   805  1417\n",
      "  1419  1420  1423  1424  1425  1821  3020  7616  9483 13276 14549 14658\n",
      " 15219 15340 15578 15659 16134 16176 16802 18779 19918 21201 21758 22534\n",
      " 22968 23592 23612 25639 25821 26510 28010 29294 31352 31353 31543 32747\n",
      " 32899 33245]\n",
      "output shape in generation is (128, 10)\n",
      "unique values [    1    64    72    73    76    79    88   117   792   805  1241  1417\n",
      "  1419  1420  1423  1424  1425  1821  3020  3610  6894  7012  7033  7339\n",
      "  7616  8204 11562 13276 13368 14118 14549 14658 15219 15340 15577 15578\n",
      " 15659 16134 16176 16802 16972 17154 17785 18804 19918 20398 21201 22201\n",
      " 22213 22230 22534 22968 23592 23612 23842 23956 24382 24452 24730 25204\n",
      " 25639 25821 25871 25881 26190 28010 28296 28732 29294 30547 31348 31352\n",
      " 31353 31514 31543 31891 32353 32747 32842 32846 32883 32899 33124 33245]\n",
      "output shape in prediction is (128, 33278)\n",
      "[VAL]  Epoch [32/20]   Loss: 5.1066\n",
      "Saving model, predictions and generated output for epoch 10 with NLL: 5.106645\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n",
      "Processed 80 batches\n",
      "Processed 90 batches\n",
      "Processed 100 batches\n",
      "Processed 110 batches\n",
      "Processed 120 batches\n",
      "Processed 130 batches\n",
      "Processed 140 batches\n",
      "Processed 150 batches\n",
      "Processed 160 batches\n",
      "[TRAIN]  Epoch [33/20]   Loss: 5.6222\n",
      "output shape in prediction is (128, 33278)\n",
      "output shape in generation is (32, 10)\n",
      "unique values [    1     4    64    72    73    76    79    87    88   805   946  1241\n",
      "  1417  1419  1420  1424  1425  1821  3020  7616  9483 13276 14549 14658\n",
      " 15219 15340 15578 15659 16134 16176 16802 18779 19918 20398 21201 22534\n",
      " 22968 25639 25821 25871 26190 28010 28296 31352 31353 31543 32747 32899\n",
      " 33245]\n",
      "output shape in generation is (128, 10)\n",
      "unique values [    1    64    72    73    76    79    88   460   805  1182  1417  1419\n",
      "  1420  1423  1424  1425  1821  2224  3020  4698  5140  7033  7339  7616\n",
      " 11367 11562 13276 13368 13774 14118 14549 14658 15219 15340 15382 15577\n",
      " 15578 15659 16134 16176 16802 16972 17154 18804 19918 20398 21201 22201\n",
      " 22213 22230 22534 22968 22969 23592 23842 23959 24343 24730 25204 25639\n",
      " 25821 25871 26190 28010 28732 29092 29294 29310 30125 30547 31348 31352\n",
      " 31353 31399 31514 31543 31891 32502 32747 32842 32899 33124]\n",
      "output shape in prediction is (128, 33278)\n",
      "[VAL]  Epoch [33/20]   Loss: 5.1358\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n",
      "Processed 80 batches\n",
      "Processed 90 batches\n",
      "Processed 100 batches\n",
      "Processed 110 batches\n",
      "Processed 120 batches\n",
      "Processed 130 batches\n",
      "Processed 140 batches\n",
      "Processed 150 batches\n",
      "Processed 160 batches\n",
      "[TRAIN]  Epoch [34/20]   Loss: 5.5870\n",
      "output shape in prediction is (128, 33278)\n",
      "output shape in generation is (32, 10)\n",
      "unique values [    1     4    72    73    76    79    87    88   805   946  1241  1414\n",
      "  1417  1419  1420  1423  1424  1425  1821  3020  7616  9483 13276 14549\n",
      " 14658 15219 15340 15578 15659 16134 16176 16802 17154 18779 19918 20398\n",
      " 21201 22346 22968 25639 25821 28010 28296 29294 31352 31353 31543 32747\n",
      " 32842 32899 33245]\n",
      "output shape in generation is (128, 10)\n",
      "unique values [    1    64    72    73    76    79    88   460   805   946  1414  1417\n",
      "  1419  1420  1423  1424  1425  1821  1890  2224  2512  3020  7033  7339\n",
      "  7616 11562 13276 13368 13774 14118 14549 14658 15219 15340 15577 15578\n",
      " 15659 16134 16176 16802 16972 17154 18804 19918 20398 21148 21201 21987\n",
      " 22201 22213 22230 22356 22968 23592 23842 23959 24343 24382 24452 24730\n",
      " 25204 25620 25639 25821 25871 25881 26190 26588 28296 28727 28732 29092\n",
      " 29294 30125 30547 31348 31352 31353 31514 31543 31891 32747 32842 32846\n",
      " 32883 32899 33124]\n",
      "output shape in prediction is (128, 33278)\n",
      "[VAL]  Epoch [34/20]   Loss: 5.0982\n",
      "Saving model, predictions and generated output for epoch 12 with NLL: 5.098227\n",
      "Processed 10 batches\n",
      "Processed 20 batches\n",
      "Processed 30 batches\n",
      "Processed 40 batches\n",
      "Processed 50 batches\n",
      "Processed 60 batches\n",
      "Processed 70 batches\n",
      "Processed 80 batches\n",
      "Processed 90 batches\n",
      "Processed 100 batches\n",
      "Processed 110 batches\n",
      "Processed 120 batches\n",
      "Processed 130 batches\n",
      "Processed 140 batches\n",
      "Processed 150 batches\n",
      "Processed 160 batches\n",
      "[TRAIN]  Epoch [35/20]   Loss: 5.5579\n",
      "output shape in prediction is (128, 33278)\n",
      "output shape in generation is (32, 10)\n",
      "unique values [    1     4    72    73    76    79    87    88   805   946  1241  1414\n",
      "  1417  1419  1420  1423  1424  1425  1821  3020  7616  9483 13276 14549\n",
      " 14658 15340 15578 15659 16134 16176 16802 18779 19918 20398 21201 22534\n",
      " 22968 23592 23612 25639 25821 28010 29294 31352 31353 31543 32747 32842\n",
      " 32899 33245]\n"
     ]
    }
   ],
   "source": [
    "best_nll = 1e30 \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    trainer.train()\n",
    "    nll = trainer.test()\n",
    "    if nll < best_nll:\n",
    "        best_nll = nll\n",
    "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
    "        trainer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2FmDqBCSwRf"
   },
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                        # Don't change these\n",
    "# plot training curves\n",
    "plt.figure()\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('NLL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipdbmqaGSwRh"
   },
   "outputs": [],
   "source": [
    "# see generated output\n",
    "print (trainer.generated[-1]) # get last generated output"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "training-HW4P1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
