{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Pu32KRrYInA",
    "outputId": "34a60c6c-8c6a-4a29-cfa3-aeff70f2f47b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 3.7.12 (default, Sep 10 2021, 00:21:48) \n",
      "[GCC 7.5.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from torch.utils import data\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "print(cuda, sys.version)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "np.random.seed(420)\n",
    "torch.manual_seed(420)\n",
    "\n",
    "LETTER_LIST = ['<sos>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \n",
    "               'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '-', \"'\", '.', '_', '+', ' ', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bR7rvCvoRSW",
    "outputId": "51cc0225-06a5-4cf8-c21b-facfd6aa6a40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive._mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FZwcniWiYPeb"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def create_dictionaries(letter_list: List[str]):# -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    '''\n",
    "    Create dictionaries for letter2index and index2letter transformations\n",
    "    '''\n",
    "    letter2index = {letter_list[i]:i for i in range(len(letter_list))}\n",
    "    index2letter = {j:i for (i,j) in letter2index.items()}\n",
    "    return letter2index, index2letter\n",
    "    \n",
    "def transform_letter_to_index(raw_transcripts,letter2index):#: List[str]) -> List[int]:\n",
    "    '''\n",
    "    Transforms text input to numerical input by converting each letter \n",
    "    to its corresponding index from letter_list\n",
    "\n",
    "    Args:\n",
    "        raw_transcripts: Raw text transcripts with the shape of (N, )\n",
    "    \n",
    "    Return:\n",
    "        transcripts: Converted index-format transcripts. This would be a list with a length of N\n",
    "    '''  \n",
    "    return [letter2index[i] for i in raw_transcripts]\n",
    "    \n",
    "# Create the letter2index and index2letter dictionary\n",
    "letter2index, index2letter = create_dictionaries(LETTER_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iKmDlrX0q-db",
    "outputId": "c85245ab-976f-41f2-c75e-fd67065fcd0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/IDL-HW4-P2/hw4p2_toy_dataset\n"
     ]
    }
   ],
   "source": [
    "cd '/content/drive/MyDrive/IDL-HW4-P2/hw4p2_toy_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lUR512b3qzh3"
   },
   "outputs": [],
   "source": [
    "# Load the training, validation and testing data\n",
    "train_data = np.load('train.npz', allow_pickle=True, encoding='bytes')\n",
    "valid_data = np.load('dev.npz', allow_pickle=True, encoding='bytes')\n",
    "# test_data = np.load('test.npy', allow_pickle=True, encoding='bytes')\n",
    "\n",
    "# Load the training, validation raw text transcripts\n",
    "raw_train_transcript = np.load('train_transcripts.npz', allow_pickle=True,encoding='bytes')\n",
    "raw_valid_transcript = np.load('dev_transcripts.npz', allow_pickle=True,encoding='bytes')\n",
    "\n",
    "# TODO: Convert the raw text transcripts into indexes\n",
    "train_transcript = [transform_letter_to_index(list(raw_train_transcript['data'][i]),letter2index) for i in range(len(raw_train_transcript['data']))] \n",
    "valid_transcript = [transform_letter_to_index(list(raw_valid_transcript['data'][i]),letter2index) for i in range(len(raw_valid_transcript['data']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LyzeuUmjuFen"
   },
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X#.astype() if you want its data type converted\n",
    "        self.Y = Y#.astype()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # For testing set, return only x\n",
    "        if self.Y is None:\n",
    "            return torch.as_tensor(self.X[index])\n",
    "        # For training and validation set, return x and y\n",
    "        else:\n",
    "            return torch.as_tensor(self.X[index]), torch.as_tensor(self.Y[index])\n",
    "\n",
    "def collate_train_val(data):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "        pad_x: the padded x (training/validation speech data) \n",
    "        pad_y: the padded y (text labels - transcripts)\n",
    "        x_len: the length of x\n",
    "        y_len: the length of y\n",
    "    \"\"\"\n",
    "    # Consider batch_first = True\n",
    "    x_batch = [x for (x,y) in data]\n",
    "    y_batch = [y for (x,y) in data]\n",
    "\n",
    "    x_len = []\n",
    "    y_len = []\n",
    "\n",
    "    for i in range(len(x_batch)):\n",
    "        x_len.append(x_batch[i].shape[0])\n",
    "    \n",
    "    for i in range(len(y_batch)):\n",
    "        y_len.append(y_batch[i].shape[0])\n",
    "\n",
    "    x_len = torch.tensor(x_len)\n",
    "    y_len = torch.tensor(y_len)\n",
    "\n",
    "    pad_x = rnn_utils.pad_sequence(x_batch,batch_first=True)\n",
    "    pad_y = rnn_utils.pad_sequence(y_batch,batch_first=True)\n",
    "    \n",
    "    return pad_x, pad_y, x_len, y_len\n",
    "    \n",
    "\n",
    "def collate_test(data):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "        pad_x: the padded x (testing speech data) \n",
    "        x_len: the length of x\n",
    "    \"\"\"\n",
    "    # Consider batch_first = True\n",
    "    x_batch = [x for (x,y) in data]\n",
    "    y_batch = [y for (x,y) in data]\n",
    "\n",
    "    x_len = []\n",
    "    y_len = []\n",
    "\n",
    "    for i in range(10):\n",
    "        x_len.append(x_batch[i].shape[0])\n",
    "    \n",
    "    for i in range(10):\n",
    "        y_len.append(y_batch[i].shape[0])\n",
    "\n",
    "    x_len = torch.tensor(x_len)\n",
    "    y_len = torch.tensor(y_len)\n",
    "\n",
    "    pad_x = rnn_utils.pad_sequence(x_batch,batch_first=True)\n",
    "    pad_y = rnn_utils.pad_sequence(y_batch,batch_first=True)\n",
    "    \n",
    "    return pad_x, pad_y, x_len, y_len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zRaNo8ycuMKm"
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = MyDataset(train_data['data'],train_transcript)\n",
    "valid_dataset = MyDataset(valid_data['data'],valid_transcript)\n",
    "test_dataset = MyDataset(valid_data['data'],valid_transcript) # fill this out\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=128,shuffle=True,num_workers=4,pin_memory=True,collate_fn=collate_train_val)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=128,shuffle=False,num_workers=4,pin_memory=True,collate_fn=collate_train_val)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=128,shuffle=False,num_workers=4,pin_memory=True,collate_fn=collate_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HZOzu9RY2DMx"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Encoder takes the utterances as inputs and returns the key, value and unpacked_x_len.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, input_dim, encoder_hidden_dim, key_value_size=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        # The first LSTM layer at the bottom\n",
    "        self.lstm = nn.LSTM(input_dim,encoder_hidden_dim,batch_first=True,num_layers=2,bidirectional=True)\n",
    "        self.key_network = nn.Linear(encoder_hidden_dim*2,key_value_size)\n",
    "        self.value_network = nn.Linear(encoder_hidden_dim*2,key_value_size)\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        \"\"\"\n",
    "        1. Pack your input and pass it through the first LSTM layer (no truncation)\n",
    "        2. Pass it through the pyramidal LSTM layer\n",
    "        3. Pad your input back to (B, T, *) or (T, B, *) shape\n",
    "        4. Output Key, Value, and truncated input lens\n",
    "\n",
    "        Key and value could be\n",
    "            (i) Concatenated hidden vectors from all time steps (key == value).\n",
    "            (ii) Linear projections of the output from the last pBLSTM network.\n",
    "                If you choose this way, you can use the final output of\n",
    "                your pBLSTM network.\n",
    "        \"\"\"\n",
    "        pack_encoder = rnn_utils.pack_padded_sequence(x,batch_first=True,enforce_sorted=False,lengths=x_len)\n",
    "\n",
    "        out_lstm, _ = self.lstm(pack_encoder)\n",
    "\n",
    "        unpack_encoder, len_encoder = rnn_utils.pad_packed_sequence(out_lstm,batch_first=True)\n",
    "\n",
    "        key = self.key_network(unpack_encoder)\n",
    "        value = self.value_network(unpack_encoder)\n",
    "        \n",
    "        return key, value, len_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pvBaxoWU6PNk"
   },
   "outputs": [],
   "source": [
    "def plot_attention(attention):\n",
    "    # utility function for debugging\n",
    "    plt.clf()\n",
    "    sns.heatmap(attention, cmap='GnBu')\n",
    "    plt.show()\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Attention is calculated using key and value from encoder and query from decoder.\n",
    "    Here are different ways to compute attention and context:\n",
    "    1. Dot-product attention\n",
    "        energy = bmm(key, query) \n",
    "        # Optional: Scaled dot-product by normalizing with sqrt key dimension\n",
    "        # Check \"attention is all you need\" Section 3.2.1\n",
    "    * 1st way is what most TAs are comfortable with, but if you want to explore...\n",
    "    2. Cosine attention\n",
    "        energy = cosine(query, key) # almost the same as dot-product xD \n",
    "    3. Bi-linear attention\n",
    "        W = Linear transformation (learnable parameter): d_k -> d_q\n",
    "        energy = bmm(key @ W, query)\n",
    "    4. Multi-layer perceptron\n",
    "        # Check \"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial\" Section 8.4\n",
    "    \n",
    "    After obtaining unnormalized attention weights (energy), compute and return attention and context, i.e.,\n",
    "    energy = mask(energy) # mask out padded elements with big negative number (e.g. -1e9)\n",
    "    attention = softmax(energy)\n",
    "    context = bmm(attention, value)\n",
    "\n",
    "    5. Multi-Head Attention\n",
    "        # Check \"attention is all you need\" Section 3.2.2\n",
    "        h = Number of heads\n",
    "        W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n",
    "        W_O: d_v -> d_v\n",
    "\n",
    "        Reshape K: (B, T, d_k)\n",
    "        to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n",
    "        Reshape V: (B, T, d_v)\n",
    "        to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n",
    "        Reshape Q: (B, d_q)\n",
    "        to (B, h, d_q // h)\n",
    "\n",
    "        energy = Q @ K^T\n",
    "        energy = mask(energy)\n",
    "        attention = softmax(energy)\n",
    "        multi_head = attention @ V\n",
    "        multi_head = multi_head reshaped to (B, d_v)\n",
    "        context = multi_head @ W_O\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        # Optional: dropout\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            key: (batch_size, seq_len, d_k)\n",
    "            value: (batch_size, seq_len, d_v)\n",
    "            query: (batch_size, d_q)\n",
    "        * Hint: d_k == d_v == d_q is often true if you use linear projections\n",
    "        return:\n",
    "            context: (batch_size, key_val_dim)\n",
    "        \n",
    "        \"\"\"\n",
    "        # query = torch.tensor(query)\n",
    "        query = torch.unsqueeze(query,dim=2)\n",
    "\n",
    "        energy = torch.unsqueeze(mask, 2) * torch.bmm(key,query)\n",
    "\n",
    "        attention = torch.softmax(energy,dim=1)\n",
    "\n",
    "        context = torch.bmm(torch.transpose(attention,1,2),value)\n",
    "\n",
    "        context = torch.squeeze(context)\n",
    "        return context, attention\n",
    "        # we return attention weights for plotting (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "y1kn2Ueg7hp_"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step.\n",
    "    Thus we use LSTMCell instead of LSTM here.\n",
    "    The output from the last LSTMCell can be used as a query for calculating attention.\n",
    "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
    "    '''\n",
    "    def __init__(self, vocab_size, decoder_hidden_dim, embed_dim, key_value_size=128):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Hint: Be careful with the padding_idx\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_dim)\n",
    "        # The number of cells is defined based on the paper\n",
    "        self.lstm1 = nn.LSTMCell(key_value_size+embed_dim,decoder_hidden_dim)\n",
    "        # self.lstm2 = nn.LSTMCell()\n",
    "    \n",
    "        self.attention = Attention()     \n",
    "        self.vocab_size = vocab_size\n",
    "        # Optional: Weight-tying\n",
    "        self.character_prob = nn.Linear(key_value_size+embed_dim,vocab_size) #: d_v -> vocab_size\n",
    "        self.key_value_size = key_value_size\n",
    "        \n",
    "        # Weight tying\n",
    "        # self.character_prob.weight = self.embedding.weight\n",
    "\n",
    "    def forward(self, key, value, encoder_len, y=None, mode='train'):\n",
    "        '''\n",
    "        Args:\n",
    "            key :(B, T, d_k) - Output of the Encoder (possibly from the Key projection layer)\n",
    "            value: (B, T, d_v) - Output of the Encoder (possibly from the Value projection layer)\n",
    "            y: (B, text_len) - Batch input of text with text_length\n",
    "            mode: Train or eval mode for teacher forcing\n",
    "        Return:\n",
    "            predictions: the character perdiction probability \n",
    "        '''\n",
    "\n",
    "        B, key_seq_max_len, key_value_size = key.shape\n",
    "\n",
    "        if mode == 'train':\n",
    "            max_len =  y.shape[1]\n",
    "            char_embeddings = self.embedding(y)\n",
    "        else:\n",
    "            max_len = 600\n",
    "\n",
    "        # TODO: Create the attention mask here (outside the for loop rather than inside) to aviod repetition\n",
    "        mask = torch.ones(B,key_seq_max_len) *1e-5\n",
    "        \n",
    "        for i in range(B):\n",
    "            mask[i,:encoder_len[i]] = 1\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        predictions = []\n",
    "        # This is the first input to the decoder\n",
    "        # What should the fill_value be?\n",
    "        prediction = torch.full((B,), fill_value= 0, device=device)\n",
    "        # The length of hidden_states vector should depend on the number of LSTM Cells defined in init\n",
    "        # The paper uses 2\n",
    "        hidden_states = [None, None] \n",
    "        \n",
    "        # TODO: Initialize the context\n",
    "        context = torch.zeros(B,key_value_size)\n",
    "        context = context.to(device)\n",
    "\n",
    "        attention_plot = [] # this is for debugging\n",
    "\n",
    "        for i in range(max_len):\n",
    "            if mode == 'train':\n",
    "                # TODO: Implement Teacher Forcing\n",
    "                \"\"\"\n",
    "                if using teacher_forcing:\n",
    "                    if i == 0:\n",
    "                        # This is the first time step\n",
    "                        # Hint: How did you initialize \"prediction\" variable above?\n",
    "                    else:\n",
    "                        # Otherwise, feed the label of the *previous* time step\n",
    "                else:\n",
    "                    char_embed = embedding of the previous prediction\n",
    "                \"\"\"     \n",
    "                if np.random.randint(0,10) >= 5:\n",
    "\n",
    "                    if i == 0:\n",
    "\n",
    "                        prediction = prediction.to(device)\n",
    "                        char_embed = self.embedding(prediction)\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        char_emb = char_embeddings[:,i,:]\n",
    "                \n",
    "                else:\n",
    "                  \n",
    "                    prediction = prediction.to(device)\n",
    "                    char_embed = self.embedding(prediction)\n",
    "            else:\n",
    "                prediction = prediction.to(device)\n",
    "                char_embed = self.embedding(prediction)\n",
    "\n",
    "            # what vectors should be concatenated as a context?\n",
    "            y_context = torch.cat([char_embed,context], dim=1)\n",
    "            # context and hidden states of lstm 1 from the previous time step should be fed\n",
    "            hidden_states,_ = self.lstm1(y_context)\n",
    "            query = hidden_states\n",
    "            \n",
    "            # Compute attention from the output of the second LSTM Cell\n",
    "            context, attention = self.attention(query, key, value, mask)\n",
    "            #print('After Attention')\n",
    "\n",
    "            # We store the first attention of this batch for debugging\n",
    "            attention_plot.append(attention[:,:,0].detach().cpu())\n",
    "          \n",
    "            output_context = torch.cat([query,context], dim=1)\n",
    "            prediction = self.character_prob(output_context)\n",
    "            # store predictions\n",
    "            predictions.append(torch.unsqueeze(prediction,dim=1))\n",
    "            prediction = torch.argmax(F.softmax(prediction,dim=1),dim=1)\n",
    "        # Concatenate the attention and predictions to return\n",
    "        attentions = torch.stack(attention_plot, dim=0)\n",
    "        predictions = torch.cat(predictions, dim=1)\n",
    "        return predictions, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3-Iof2qrMtwF"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    '''\n",
    "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
    "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, vocab_size, encoder_hidden_dim, decoder_hidden_dim, embed_dim, key_value_size=128):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder = Encoder(input_dim,encoder_hidden_dim,key_value_size)\n",
    "        self.decoder = Decoder(vocab_size,decoder_hidden_dim,embed_dim,key_value_size)\n",
    "\n",
    "    def forward(self, x, x_len, y=None, mode='train'):\n",
    "        key, value, encoder_len = self.encoder(x, x_len)\n",
    "        predictions = self.decoder(key, value, encoder_len, y=y, mode=mode) \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PDKbvIi-NUMr"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, mode):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, y, x_len, y_len = batch\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        predictions, attentions = model(x, x_len, y, mode=mode)\n",
    "        \n",
    "        # Generate a mask based on target length. This is to mark padded elements\n",
    "        # so that we can exclude them from computing loss\n",
    "        mask = torch.ones(y.shape[0],y.shape[1])*1e-5\n",
    "        \n",
    "        for i in range(y.shape[0]):\n",
    "            mask[i,:y_len[i]] = 1\n",
    "        mask = torch.flatten(mask)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        predictions = torch.flatten(predictions,start_dim=0,end_dim=1)\n",
    "        y = torch.flatten(y)\n",
    "\n",
    "        # Make sure you have the correct shape of predictions when putting into criterion\n",
    "        loss = criterion(predictions, y)\n",
    "        # Use the mask you defined above to compute the average loss\n",
    "        masked_loss = torch.mean(loss*mask)\n",
    "\n",
    "        if (i+1)%5 == 0:\n",
    "            print(\"Batch: {}, Loss: {}\".format(i+1,masked_loss.item())) \n",
    "\n",
    "        # backprop\n",
    "        running_loss += masked_loss.item() \n",
    "        # Optional: Gradient clipping\n",
    "        masked_loss.backward()\n",
    "        # When computing Levenshtein distance, make sure you truncate prediction/target\n",
    "        optimizer.step()\n",
    "        # Optional: plot your attention for debugging\n",
    "        # plot_attention(attentions)\n",
    "    print(\"Total Loss: \",running_loss/len(train_loader))\n",
    "    plot_attention(attentions[:,0,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Do0JZ_8GREcb"
   },
   "outputs": [],
   "source": [
    "# TODO: Define your model and put it on the device here\n",
    "# ...\n",
    "model = Seq2Seq(input_dim=40,vocab_size=len(LETTER_LIST),encoder_hidden_dim=512,decoder_hidden_dim=512,embed_dim=512,key_value_size=512)\n",
    "model = model.to('cuda')\n",
    "n_epochs = 100\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer,step_size = 10,gamma=0.5,verbose=True)\n",
    "# Make sure you understand the implication of setting reduction = 'none'\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "mode = 'train'\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch:{}\\n\".format(epoch))\n",
    "    train(model, train_loader, criterion, optimizer, mode)\n",
    "    PATH = \"/content/drive/MyDrive/IDL-HW4-P2/saved_model_hw4P2/S2S_Model_{}.pt\".format(epoch)\n",
    "    val_loss = val(model, valid_loader)\n",
    "    torch.save({'epoch': epoch,\n",
    "               'model_state_dict':model.state_dict(),\n",
    "               'scheduler_state_dict':scheduler.state_dict(),\n",
    "               'optimizer_state_dict':optimizer.state_dict(),\n",
    "               'val_loss':val_loss},PATH)\n",
    "              \n",
    "              \n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvkuXcRR9-2I",
    "outputId": "7cc3f751-2577-44fb-f2ef-f34b31f80637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(input_dim=40,vocab_size=len(LETTER_LIST),encoder_hidden_dim=512,decoder_hidden_dim=512,embed_dim=512,key_value_size=512)\n",
    "model = model.to('cuda')\n",
    "n_epochs = 100\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer,step_size = 10,gamma=0.5,verbose=True)\n",
    "# Make sure you understand the implication of setting reduction = 'none'\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "mode = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "EPtHp2qhia9O"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/content/drive/MyDrive/IDL-HW4-P2/saved_model_hw4P2/S2S_Model_83.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "epochs = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "IQusN49FESab"
   },
   "outputs": [],
   "source": [
    "def val_func(model, valid_loader):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(valid_loader):\n",
    "\n",
    "        x, y, x_len, y_len = batch\n",
    "        x = x[0:10]\n",
    "        y = y[0:10]\n",
    "        x_len = x_len[0:10]\n",
    "        print('lengths', x_len)\n",
    "        y_len = y_len[0:10]\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        predictions, attentions = model(x, x_len, y, mode=mode)\n",
    "\n",
    "\n",
    "        break\n",
    "    print(attentions.shape)\n",
    "    final_attn = []\n",
    "    for i in range(10):\n",
    "        print(attentions[0:x_len[i], i, 0:x_len[i]].shape)\n",
    "        final_attn.append(np.transpose(attentions[0:x_len[i], i, 0:x_len[i]].cpu().detach().numpy()))\n",
    "\n",
    "    final_attn_new = np.array(final_attn)\n",
    "    print(final_attn_new[0].shape)\n",
    "    np.save('/content/drive/MyDrive/IDL-HW4-P2/handin/attention.npy', final_attn_new)\n",
    "    print(\"Total Validation Loss: \",running_loss/len(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4MnIrkS9z8X",
    "outputId": "fe6808a3-aaa2-410d-9bb0-bbf67efdb87e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengths tensor([29, 21, 21, 31, 26, 31, 20, 18, 34, 15])\n",
      "torch.Size([600, 10, 34])\n",
      "torch.Size([29, 29])\n",
      "torch.Size([21, 21])\n",
      "torch.Size([21, 21])\n",
      "torch.Size([31, 31])\n",
      "torch.Size([26, 26])\n",
      "torch.Size([31, 31])\n",
      "torch.Size([20, 20])\n",
      "torch.Size([18, 18])\n",
      "torch.Size([34, 34])\n",
      "torch.Size([15, 15])\n",
      "(29, 29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Validation Loss:  0.0\n"
     ]
    }
   ],
   "source": [
    "val_func(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nkb3p9VNGAXM",
    "outputId": "b41cad01-7840-435d-8f90-1d6de27ce0fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/IDL-HW4-P2\n"
     ]
    }
   ],
   "source": [
    "# cd /content/drive/MyDrive/IDL-HW4-P2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "O2wWbhF8-G7r"
   },
   "outputs": [],
   "source": [
    "# !tar -cf /content/drive/MyDrive/IDL-HW4-P2/handin.tar ./handin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wR979bqgGzde"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "HW4P2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
